{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0de36b31320ba4c88b4f85a74724f3d16c36a44df48581253710b1065e752d9e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Code pulled from\n",
    "\n",
    "https://console.paperspace.com/gcn-team/notebook/pr5ddt1g9?file=mario_notebook.ipynb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action) # default rewards from gym\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear past frame buffer and init to first obs\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Downsamples image to 84x84\n",
    "    Greyscales image\n",
    "\n",
    "    Returns numpy array\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 240 * 256 * 3:\n",
    "            img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    \"\"\"Normalize pixel values in frame --> 0 to 1\"\"\"\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "def make_env(env):\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    return JoypadSpace(env, RIGHT_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSolver(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQNSolver, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "    \n",
    "\n",
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_space, action_space, max_memory_size, batch_size, gamma, lr,\n",
    "                 dropout, exploration_max, exploration_min, exploration_decay, double_dq, pretrained):\n",
    "\n",
    "        # Define DQN Layers\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.double_dq = double_dq\n",
    "        self.pretrained = pretrained\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        if self.double_dq:  \n",
    "            self.local_net = DQNSolver(state_space, action_space).to(self.device)\n",
    "            self.target_net = DQNSolver(state_space, action_space).to(self.device)\n",
    "            \n",
    "            if self.pretrained:\n",
    "                self.local_net.load_state_dict(torch.load(\"dq1.pt\", map_location=torch.device(self.device)))\n",
    "                self.target_net.load_state_dict(torch.load(\"dq2.pt\", map_location=torch.device(self.device)))\n",
    "                    \n",
    "            self.optimizer = torch.optim.Adam(self.local_net.parameters(), lr=lr)\n",
    "            self.copy = 5000  # Copy the local model weights into the target network every 5000 steps\n",
    "            self.step = 0\n",
    "        else:  \n",
    "            self.dqn = DQNSolver(state_space, action_space).to(self.device)\n",
    "            \n",
    "            if self.pretrained:\n",
    "                self.dqn.load_state_dict(torch.load(\"dq.pt\", map_location=torch.device(self.device)))\n",
    "            self.optimizer = torch.optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "        # Create memory\n",
    "        self.max_memory_size = max_memory_size\n",
    "        if self.pretrained:\n",
    "            self.STATE_MEM = torch.load(\"STATE_MEM.pt\")\n",
    "            self.ACTION_MEM = torch.load(\"ACTION_MEM.pt\")\n",
    "            self.REWARD_MEM = torch.load(\"REWARD_MEM.pt\")\n",
    "            self.STATE2_MEM = torch.load(\"STATE2_MEM.pt\")\n",
    "            self.DONE_MEM = torch.load(\"DONE_MEM.pt\")\n",
    "            with open(\"ending_position.pkl\", 'rb') as f:\n",
    "                self.ending_position = pickle.load(f)\n",
    "            with open(\"num_in_queue.pkl\", 'rb') as f:\n",
    "                self.num_in_queue = pickle.load(f)\n",
    "        else:\n",
    "            self.STATE_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "            self.ACTION_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.REWARD_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.STATE2_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "            self.DONE_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.ending_position = 0\n",
    "            self.num_in_queue = 0\n",
    "        \n",
    "        self.memory_sample_size = batch_size\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.gamma = gamma\n",
    "        self.l1 = nn.SmoothL1Loss().to(self.device) # Also known as Huber loss\n",
    "        self.exploration_max = exploration_max\n",
    "        self.exploration_rate = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.exploration_decay = exploration_decay\n",
    "\n",
    "    def remember(self, state, action, reward, state2, done):\n",
    "        self.STATE_MEM[self.ending_position] = state.float()\n",
    "        self.ACTION_MEM[self.ending_position] = action.float()\n",
    "        self.REWARD_MEM[self.ending_position] = reward.float()\n",
    "        self.STATE2_MEM[self.ending_position] = state2.float()\n",
    "        self.DONE_MEM[self.ending_position] = done.float()\n",
    "        self.ending_position = (self.ending_position + 1) % self.max_memory_size  # FIFO tensor\n",
    "        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n",
    "        \n",
    "    def recall(self):\n",
    "        # Randomly sample 'batch size' experiences\n",
    "        idx = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n",
    "        \n",
    "        STATE = self.STATE_MEM[idx]\n",
    "        ACTION = self.ACTION_MEM[idx]\n",
    "        REWARD = self.REWARD_MEM[idx]\n",
    "        STATE2 = self.STATE2_MEM[idx]\n",
    "        DONE = self.DONE_MEM[idx]\n",
    "        \n",
    "        return STATE, ACTION, REWARD, STATE2, DONE\n",
    "\n",
    "    def act(self, state):\n",
    "        # Epsilon-greedy action\n",
    "        \n",
    "        if self.double_dq:\n",
    "            self.step += 1\n",
    "        if random.random() < self.exploration_rate:  \n",
    "            return torch.tensor([[random.randrange(self.action_space)]])\n",
    "        if self.double_dq:\n",
    "            # Local net is used for the policy\n",
    "            return torch.argmax(self.local_net(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
    "        else:\n",
    "            return torch.argmax(self.dqn(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
    "\n",
    "    def copy_model(self):\n",
    "        # Copy local net weights into target net\n",
    "        \n",
    "        self.target_net.load_state_dict(self.local_net.state_dict())\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \n",
    "        if self.double_dq and self.step % self.copy == 0:\n",
    "            self.copy_model()\n",
    "\n",
    "        if self.memory_sample_size > self.num_in_queue:\n",
    "            return\n",
    "\n",
    "        STATE, ACTION, REWARD, STATE2, DONE = self.recall()\n",
    "        STATE = STATE.to(self.device)\n",
    "        ACTION = ACTION.to(self.device)\n",
    "        REWARD = REWARD.to(self.device)\n",
    "        STATE2 = STATE2.to(self.device)\n",
    "        DONE = DONE.to(self.device)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        if self.double_dq:\n",
    "            # Double Q-Learning target is Q*(S, A) <- r + γ max_a Q_target(S', a)\n",
    "            target = REWARD + torch.mul((self.gamma * \n",
    "                                        self.target_net(STATE2).max(1).values.unsqueeze(1)), \n",
    "                                        1 - DONE)\n",
    "\n",
    "            current = self.local_net(STATE).gather(1, ACTION.long()) # Local net approximation of Q-value\n",
    "        else:\n",
    "            # Q-Learning target is Q*(S, A) <- r + γ max_a Q(S', a) \n",
    "            target = REWARD + torch.mul((self.gamma * \n",
    "                                        self.dqn(STATE2).max(1).values.unsqueeze(1)), \n",
    "                                        1 - DONE)\n",
    "                \n",
    "            current = self.dqn(STATE).gather(1, ACTION.long())\n",
    "        \n",
    "        loss = self.l1(current, target)\n",
    "        loss.backward() # Compute gradients\n",
    "        self.optimizer.step() # Backpropagate error\n",
    "\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        \n",
    "        # Makes sure that exploration rate is always at least 'exploration min'\n",
    "        self.exploration_rate = max(self.exploration_rate, self.exploration_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_action(action, action_space):\n",
    "    # Given a scalar action, return a one-hot encoded action\n",
    "    \n",
    "    return [0 for _ in range(action)] + [1] + [0 for _ in range(action + 1, action_space)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, ep=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Episode: %d %s\" % (ep, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(training_mode, pretrained):\n",
    "   \n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "    env = make_env(env)  # Wraps the environment so that frames are grayscale \n",
    "    observation_space = env.observation_space.shape\n",
    "    action_space = env.action_space.n\n",
    "    agent = DQNAgent(state_space=observation_space,\n",
    "                     action_space=action_space,\n",
    "                     max_memory_size=30000,\n",
    "                     batch_size=32,\n",
    "                     gamma=0.90,\n",
    "                     lr=0.00025,\n",
    "                     dropout=0.,\n",
    "                     exploration_max=1.0,\n",
    "                     exploration_min=0.02,\n",
    "                     exploration_decay=0.99,\n",
    "                     double_dq=True,\n",
    "                     pretrained=pretrained)\n",
    "    \n",
    "    num_episodes = 100\n",
    "    env.reset()\n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        state = torch.Tensor([state])\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            if not training_mode:\n",
    "                show_state(env, ep_num)\n",
    "            action = agent.act(state)\n",
    "            steps += 1\n",
    "            \n",
    "            state_next, reward, terminal, info = env.step(int(action[0]))\n",
    "            total_reward += reward\n",
    "            state_next = torch.Tensor([state_next])\n",
    "            reward = torch.tensor([reward]).unsqueeze(0)\n",
    "            \n",
    "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "            \n",
    "            if training_mode:\n",
    "                agent.remember(state, action, reward, state_next, terminal)\n",
    "                agent.experience_replay()\n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "        print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "        num_episodes += 1      \n",
    "    \n",
    "    if training_mode:\n",
    "        with open(\"ending_position.pkl\", \"wb\") as f:\n",
    "            pickle.dump(agent.ending_position, f)\n",
    "        with open(\"num_in_queue.pkl\", \"wb\") as f:\n",
    "            pickle.dump(agent.num_in_queue, f)\n",
    "        with open(\"total_rewards.pkl\", \"wb\") as f:\n",
    "            pickle.dump(total_rewards, f)\n",
    "        if agent.double_dq:\n",
    "            torch.save(agent.local_net.state_dict(), \"dq1.pt\")\n",
    "            torch.save(agent.target_net.state_dict(), \"dq2.pt\")\n",
    "        else:\n",
    "            torch.save(agent.dqn.state_dict(), \"dq.pt\")  \n",
    "        torch.save(agent.STATE_MEM,  \"STATE_MEM.pt\")\n",
    "        torch.save(agent.ACTION_MEM, \"ACTION_MEM.pt\")\n",
    "        torch.save(agent.REWARD_MEM, \"REWARD_MEM.pt\")\n",
    "        torch.save(agent.STATE2_MEM, \"STATE2_MEM.pt\")\n",
    "        torch.save(agent.DONE_MEM,   \"DONE_MEM.pt\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    if num_episodes > 500:\n",
    "        plt.title(\"Episodes trained vs. Average Rewards (per 500 eps)\")\n",
    "        plt.plot([0 for _ in range(500)] + \n",
    "                 np.convolve(total_rewards, np.ones((500,))/500, mode=\"valid\").tolist())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  1%|          | 1/100 [00:11<19:46, 11.98s/it]Total reward after episode 1 is 242.0\n",
      "  2%|▏         | 2/100 [00:29<22:04, 13.52s/it]Total reward after episode 2 is 736.0\n",
      "  3%|▎         | 3/100 [00:32<17:05, 10.57s/it]Total reward after episode 3 is 250.0\n",
      "  4%|▍         | 4/100 [00:41<16:03, 10.04s/it]Total reward after episode 4 is 606.0\n",
      "  5%|▌         | 5/100 [01:08<23:59, 15.15s/it]Total reward after episode 5 is 568.0\n",
      "  6%|▌         | 6/100 [01:26<25:07, 16.04s/it]Total reward after episode 6 is 608.0\n",
      "  7%|▋         | 7/100 [01:29<18:51, 12.17s/it]Total reward after episode 7 is 250.0\n",
      "  8%|▊         | 8/100 [01:33<14:29,  9.45s/it]Total reward after episode 8 is 250.0\n",
      "  9%|▉         | 9/100 [01:36<11:30,  7.59s/it]Total reward after episode 9 is 251.0\n",
      " 10%|█         | 10/100 [01:39<09:22,  6.25s/it]Total reward after episode 10 is 250.0\n",
      " 11%|█         | 11/100 [01:42<07:56,  5.35s/it]Total reward after episode 11 is 250.0\n",
      " 12%|█▏        | 12/100 [01:45<06:47,  4.64s/it]Total reward after episode 12 is 250.0\n",
      " 13%|█▎        | 13/100 [01:48<06:02,  4.16s/it]Total reward after episode 13 is 250.0\n",
      " 14%|█▍        | 14/100 [02:09<13:12,  9.22s/it]Total reward after episode 14 is 771.0\n",
      " 15%|█▌        | 15/100 [02:13<10:34,  7.46s/it]Total reward after episode 15 is 250.0\n",
      " 16%|█▌        | 16/100 [02:36<17:00, 12.14s/it]Total reward after episode 16 is 1029.0\n",
      " 17%|█▋        | 17/100 [02:38<12:53,  9.32s/it]Total reward after episode 17 is 248.0\n",
      " 18%|█▊        | 18/100 [02:43<10:58,  8.03s/it]Total reward after episode 18 is 250.0\n",
      " 19%|█▉        | 19/100 [03:02<15:10, 11.24s/it]Total reward after episode 19 is 633.0\n",
      " 20%|██        | 20/100 [03:05<11:44,  8.81s/it]Total reward after episode 20 is 250.0\n",
      " 21%|██        | 21/100 [03:09<09:39,  7.33s/it]Total reward after episode 21 is 250.0\n",
      " 22%|██▏       | 22/100 [03:12<07:54,  6.09s/it]Total reward after episode 22 is 250.0\n",
      " 23%|██▎       | 23/100 [03:16<06:47,  5.29s/it]Total reward after episode 23 is 249.0\n",
      " 24%|██▍       | 24/100 [03:19<05:49,  4.60s/it]Total reward after episode 24 is 251.0\n",
      " 25%|██▌       | 25/100 [03:22<05:11,  4.16s/it]Total reward after episode 25 is 250.0\n",
      " 26%|██▌       | 26/100 [03:25<04:49,  3.91s/it]Total reward after episode 26 is 250.0\n",
      " 27%|██▋       | 27/100 [03:28<04:30,  3.71s/it]Total reward after episode 27 is 250.0\n",
      " 28%|██▊       | 28/100 [03:31<04:12,  3.50s/it]Total reward after episode 28 is 250.0\n",
      " 29%|██▉       | 29/100 [03:35<04:03,  3.44s/it]Total reward after episode 29 is 250.0\n",
      " 30%|███       | 30/100 [03:38<03:56,  3.38s/it]Total reward after episode 30 is 251.0\n",
      " 31%|███       | 31/100 [03:42<03:56,  3.43s/it]Total reward after episode 31 is 250.0\n",
      " 32%|███▏      | 32/100 [03:45<03:52,  3.41s/it]Total reward after episode 32 is 250.0\n",
      " 33%|███▎      | 33/100 [03:49<03:55,  3.51s/it]Total reward after episode 33 is 250.0\n",
      " 34%|███▍      | 34/100 [03:52<03:57,  3.60s/it]Total reward after episode 34 is 240.0\n",
      " 35%|███▌      | 35/100 [03:55<03:41,  3.41s/it]Total reward after episode 35 is 250.0\n",
      " 36%|███▌      | 36/100 [03:58<03:32,  3.31s/it]Total reward after episode 36 is 251.0\n",
      " 37%|███▋      | 37/100 [04:02<03:25,  3.26s/it]Total reward after episode 37 is 251.0\n",
      " 38%|███▊      | 38/100 [04:05<03:16,  3.16s/it]Total reward after episode 38 is 250.0\n",
      " 39%|███▉      | 39/100 [04:26<08:53,  8.74s/it]Total reward after episode 39 is 628.0\n",
      " 40%|████      | 40/100 [04:29<07:04,  7.07s/it]Total reward after episode 40 is 250.0\n",
      " 41%|████      | 41/100 [04:34<06:05,  6.19s/it]Total reward after episode 41 is 249.0\n",
      " 42%|████▏     | 42/100 [04:52<09:26,  9.77s/it]Total reward after episode 42 is 607.0\n",
      " 43%|████▎     | 43/100 [04:55<07:30,  7.91s/it]Total reward after episode 43 is 251.0\n",
      " 44%|████▍     | 44/100 [05:00<06:26,  6.90s/it]Total reward after episode 44 is 236.0\n",
      " 45%|████▌     | 45/100 [05:03<05:14,  5.72s/it]Total reward after episode 45 is 249.0\n",
      " 46%|████▌     | 46/100 [05:06<04:32,  5.04s/it]Total reward after episode 46 is 246.0\n",
      " 47%|████▋     | 47/100 [05:09<03:57,  4.47s/it]Total reward after episode 47 is 249.0\n",
      " 48%|████▊     | 48/100 [05:13<03:38,  4.20s/it]Total reward after episode 48 is 244.0\n",
      " 49%|████▉     | 49/100 [05:16<03:16,  3.86s/it]Total reward after episode 49 is 250.0\n",
      " 50%|█████     | 50/100 [05:20<03:09,  3.80s/it]Total reward after episode 50 is 240.0\n",
      " 51%|█████     | 51/100 [05:23<02:56,  3.61s/it]Total reward after episode 51 is 250.0\n",
      " 52%|█████▏    | 52/100 [05:26<02:49,  3.54s/it]Total reward after episode 52 is 251.0\n",
      " 53%|█████▎    | 53/100 [05:44<06:10,  7.88s/it]Total reward after episode 53 is 809.0\n",
      " 54%|█████▍    | 54/100 [05:47<04:55,  6.43s/it]Total reward after episode 54 is 251.0\n",
      " 55%|█████▌    | 55/100 [05:55<05:10,  6.91s/it]Total reward after episode 55 is 633.0\n",
      " 56%|█████▌    | 56/100 [06:00<04:30,  6.16s/it]Total reward after episode 56 is 249.0\n",
      " 57%|█████▋    | 57/100 [06:04<03:58,  5.56s/it]Total reward after episode 57 is 250.0\n",
      " 58%|█████▊    | 58/100 [06:11<04:15,  6.08s/it]Total reward after episode 58 is 613.0\n",
      " 59%|█████▉    | 59/100 [06:14<03:32,  5.19s/it]Total reward after episode 59 is 251.0\n",
      " 60%|██████    | 60/100 [06:18<03:06,  4.66s/it]Total reward after episode 60 is 249.0\n",
      " 61%|██████    | 61/100 [06:21<02:40,  4.12s/it]Total reward after episode 61 is 249.0\n",
      " 62%|██████▏   | 62/100 [06:24<02:25,  3.82s/it]Total reward after episode 62 is 251.0\n",
      " 63%|██████▎   | 63/100 [06:27<02:16,  3.68s/it]Total reward after episode 63 is 250.0\n",
      " 64%|██████▍   | 64/100 [06:30<02:06,  3.50s/it]Total reward after episode 64 is 250.0\n",
      " 65%|██████▌   | 65/100 [06:33<01:57,  3.34s/it]Total reward after episode 65 is 248.0\n",
      " 66%|██████▌   | 66/100 [06:43<03:01,  5.34s/it]Total reward after episode 66 is 625.0\n",
      " 67%|██████▋   | 67/100 [06:46<02:32,  4.63s/it]Total reward after episode 67 is 250.0\n",
      " 68%|██████▊   | 68/100 [06:49<02:11,  4.10s/it]Total reward after episode 68 is 249.0\n",
      " 69%|██████▉   | 69/100 [06:52<01:57,  3.78s/it]Total reward after episode 69 is 250.0\n",
      " 70%|███████   | 70/100 [06:55<01:44,  3.48s/it]Total reward after episode 70 is 248.0\n",
      " 71%|███████   | 71/100 [07:05<02:35,  5.35s/it]Total reward after episode 71 is 605.0\n",
      " 72%|███████▏  | 72/100 [07:08<02:10,  4.66s/it]Total reward after episode 72 is 250.0\n",
      " 73%|███████▎  | 73/100 [07:11<01:52,  4.18s/it]Total reward after episode 73 is 250.0\n",
      " 74%|███████▍  | 74/100 [07:14<01:39,  3.84s/it]Total reward after episode 74 is 250.0\n",
      " 75%|███████▌  | 75/100 [07:17<01:29,  3.60s/it]Total reward after episode 75 is 250.0\n",
      " 76%|███████▌  | 76/100 [07:20<01:24,  3.52s/it]Total reward after episode 76 is 250.0\n",
      " 77%|███████▋  | 77/100 [07:41<03:22,  8.82s/it]Total reward after episode 77 is 625.0\n",
      " 78%|███████▊  | 78/100 [07:48<03:01,  8.27s/it]Total reward after episode 78 is 609.0\n",
      " 79%|███████▉  | 79/100 [07:51<02:19,  6.66s/it]Total reward after episode 79 is 248.0\n",
      " 80%|████████  | 80/100 [07:54<01:51,  5.58s/it]Total reward after episode 80 is 249.0\n",
      " 81%|████████  | 81/100 [07:58<01:34,  4.98s/it]Total reward after episode 81 is 249.0\n",
      " 82%|████████▏ | 82/100 [08:12<02:19,  7.76s/it]Total reward after episode 82 is 816.0\n",
      " 83%|████████▎ | 83/100 [08:29<02:56, 10.39s/it]Total reward after episode 83 is 809.0\n",
      " 84%|████████▍ | 84/100 [08:32<02:14,  8.40s/it]Total reward after episode 84 is 231.0\n",
      " 85%|████████▌ | 85/100 [08:36<01:42,  6.85s/it]Total reward after episode 85 is 246.0\n",
      " 86%|████████▌ | 86/100 [08:38<01:18,  5.62s/it]Total reward after episode 86 is 249.0\n",
      " 87%|████████▋ | 87/100 [08:57<02:03,  9.48s/it]Total reward after episode 87 is 591.0\n",
      " 88%|████████▊ | 88/100 [09:00<01:29,  7.47s/it]Total reward after episode 88 is 248.0\n",
      " 89%|████████▉ | 89/100 [09:02<01:07,  6.09s/it]Total reward after episode 89 is 250.0\n",
      " 90%|█████████ | 90/100 [09:06<00:52,  5.26s/it]Total reward after episode 90 is 249.0\n",
      " 91%|█████████ | 91/100 [09:16<01:00,  6.67s/it]Total reward after episode 91 is 605.0\n",
      " 92%|█████████▏| 92/100 [09:30<01:11,  8.96s/it]Total reward after episode 92 is 638.0\n",
      " 93%|█████████▎| 93/100 [09:49<01:23, 11.95s/it]Total reward after episode 93 is 805.0\n",
      " 94%|█████████▍| 94/100 [09:58<01:06, 11.02s/it]Total reward after episode 94 is 621.0\n",
      " 95%|█████████▌| 95/100 [10:05<00:49, 10.00s/it]Total reward after episode 95 is 610.0\n",
      " 96%|█████████▌| 96/100 [10:08<00:31,  7.88s/it]Total reward after episode 96 is 252.0\n",
      " 97%|█████████▋| 97/100 [10:28<00:34, 11.51s/it]Total reward after episode 97 is 1338.0\n",
      " 98%|█████████▊| 98/100 [10:31<00:18,  9.02s/it]Total reward after episode 98 is 251.0\n",
      " 99%|█████████▉| 99/100 [10:35<00:07,  7.29s/it]Total reward after episode 99 is 247.0\n",
      "100%|██████████| 100/100 [10:38<00:00,  6.39s/it]Total reward after episode 100 is 249.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run(training_mode=True, pretrained=False)"
   ]
  }
 ]
}